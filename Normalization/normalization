Normalization in data preprocessing / machine learning means scaling numerical data into a specific range (usually 0 to 1) without distorting the differences in the values.

ğŸ”¹ Formal Definition:

Normalization is the process of transforming data values into a common scale so that they can be compared meaningfully.

ğŸ”¹ Why we need it?

Different features may have very different scales (e.g., salary in lakhs vs. age in years).

Many ML algorithms (KNN, Neural Networks, Gradient Descentâ€“based models) perform better and converge faster when data is normalized.

Prevents large-scale features from dominating small-scale features.

ğŸ”¹ Common Normalization Techniques:

Min-Max Normalization (Rescaling)

ğ‘‹
ğ‘›
ğ‘œ
ğ‘Ÿ
ğ‘š
=
ğ‘‹
âˆ’
ğ‘‹
ğ‘š
ğ‘–
ğ‘›
ğ‘‹
ğ‘š
ğ‘
ğ‘¥
âˆ’
ğ‘‹
ğ‘š
ğ‘–
ğ‘›
X
norm
	â€‹

=
X
max
	â€‹

âˆ’X
min
	â€‹

Xâˆ’X
min
	â€‹

	â€‹


Scales data into [0,1] range.

Z-score Normalization (Standardization)

ğ‘‹
ğ‘ 
ğ‘¡
ğ‘‘
=
ğ‘‹
âˆ’
ğœ‡
ğœ
X
std
	â€‹

=
Ïƒ
Xâˆ’Î¼
	â€‹


Mean becomes 0, standard deviation becomes 1.

Decimal Scaling
Moves the decimal point of values until they fall into a range like [-1, 1].

ğŸ”¹ Example:

Original data: [20, 40, 60, 80, 100]

Min-Max Normalization (0â€“1 scale): [0.0, 0.25, 0.5, 0.75, 1.0]

Z-score Normalization: [-1.26, -0.63, 0, 0.63, 1.26]
