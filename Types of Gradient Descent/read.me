# Types of Gradient Descent

This folder contains explanations and implementations of the **different types of Gradient Descent**, which are optimization methods used to train machine learning models.

---

## ğŸ“Œ What Is Gradient Descent?

Gradient Descent is an iterative optimization algorithm used to **minimize a loss (cost) function** by updating model parameters in the direction of the negative gradient. :contentReference[oaicite:0]{index=0}

---

## ğŸ“Š Types of Gradient Descent

There are **three common variants**:

### âœ… Batch Gradient Descent
Uses the **entire training dataset** to compute the gradient and update parameters once per epoch.  
âœ” Stable but slower on large datasets. :contentReference[oaicite:1]{index=1}

### âœ… Stochastic Gradient Descent (SGD)
Updates parameters **after each training example**.  
âœ” Faster and can escape local minima, but updates are noisy. :contentReference[oaicite:2]{index=2}

### âœ… Mini-Batch Gradient Descent
Splits the dataset into **small batches**, updating parameters after each batch.  
âœ” Combines stability and speed, and is commonly used in practice. :contentReference[oaicite:3]{index=3}

---

## ğŸ“¦ What You Will Find Here

- Explanations of each type  
- Example implementations  
- Visualizations showing how each gradient descent method performs

---

## ğŸ›  How to Use

1. Open the notebooks or scripts in this folder.  
2. Run the code step by step.  
3. Observe how parameter updates differ for each method.

---

## â­ Note

Understanding these methods helps you choose the right optimization strategy when training models like linear regression, logistic regression, or neural networks.

